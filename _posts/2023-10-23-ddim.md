---
layout: post
title:  "Denoising diffusion implicit models: fast sampling for diffusion models"
date:   2023-10-23 20:00:00 -0700
---

Diffusion models are a powerful class of deep generative models. When training a diffusion model, data is progressively corrupted by noise, and the model learns to reverse the corruption process. When sampling, one starts with noise, and iteratively removes noise until one is left with data. This sampling process involves many denoising iterations and is therefore slow. [Song et al. (2021)](https://arxiv.org/abs/2010.02502) introduced a method to greatly speed up the sampling process. In this post, I'll start with a review of the standard denoising diffusion probabilistic model (DDPM) of [Ho et al. (2020)](https://arxiv.org/abs/2006.11239). Then, I'll show how training a DDPM is equivalent to training the alternative denoising diffusion implicit model (DDIM) of Song et al. (2021), which is formulated to enable faster sampling.

## Denoising diffusion probabilistic models

DDPM is a latent variable model where $$\mathbf{x}_0$$ is the observed variable and $$\mathbf{x}_1$$, ..., $$\mathbf{x}_T$$ are latent variables with the same dimensionality as $$\mathbf{x}_0$$. The latent variables are noisy versions of $$\mathbf{x}_0$$ obtained through the "forward process". This is a Markov chain, where at any given step $$t$$ in the chain, the previous variable $$\mathbf{x}_{t-1}$$ is scaled down towards $$0$$ and Gaussian white noise is added:

$$
\begin{equation}
\tag{1}

q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\sqrt{\alpha_t} \mathbf{x}_{t-1}, (1 - \alpha_t)\mathbf{I})

\end{equation}
$$

where $$\alpha_t$$ is a hyperparameter between $$0$$ and $$1$$ that controls how quickly the data is corrupted by noise. Using $$\mathbf{x}_{1:T}$$ to denote the collection of $$\mathbf{x}_1$$, ..., $$\mathbf{x}_T$$, the posterior is then:

$$
\begin{equation}
\tag{2}

q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) = \prod_{t=1}^{T} q(\mathbf{x}_t \mid \mathbf{x}_{t-1})

\end{equation}
$$

Note that the posterior is fixed and has no trainable parameters. Training involves learning the parameters of the "reverse process", which seeks to generate data from noise. This is another Markov chain that runs in the reverse direction, iteratively removing noise:

$$
\begin{equation}
\tag{3}

p_{\theta}(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}) = \mathcal{N}(\boldsymbol{\mu}_{\theta}(\mathbf{x}_{t}, t), \sigma^{2}_t \mathbf{I})

\end{equation}
$$

where $$\boldsymbol{\mu}_{\theta}$$ is the model's prediction of $$\mathbf{x}_{t-1}$$ given $$\mathbf{x}_{t}$$, $$\theta$$ is the model parameters, and $$\sigma_t$$ is a hyperparameter. The reverse process starts at $$p(\mathbf{x}_{T}) = \mathcal{N}(\mathbf{0}, \mathbf{I})$$, so the joint distribution is

$$
\begin{equation}
\tag{4}

p_{\theta}(\mathbf{x}_{0:T}) = p(\mathbf{x}_{T}) \prod_{t=1}^{T} p_{\theta}(\mathbf{x}_{t-1} \mid \mathbf{x}_{t})

\end{equation}
$$

Next, we'll derive the loss function for training the model. First, let's consider the negative evidence lower bound (ELBO) $$L$$ (which is an upper bound on the negative log likelihood $$-\log p_{\theta}(\mathbf{x}_0$$):

$$
\begin{align}

L &= \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_{0})} \left[ \log \frac{q(\mathbf{x}_{1:T} \mid \mathbf{x}_{0})}{p_{\theta}(\mathbf{x}_{0:T})} \right] \\
&= \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_{0})} \left[ \log \frac{q(\mathbf{x}_1 \mid \mathbf{x}_0)}{p(\mathbf{x}_{T}) p_{\theta}(\mathbf{x}_0 \mid \mathbf{x}_1)} \prod_{t=2}^{T} \frac{q(\mathbf{x}_t \mid \mathbf{x}_{t-1})}{p_{\theta}(\mathbf{x}_{t-1} \mid \mathbf{x}_{t})} \right] \tag{5}

\end{align}
$$

Using the fact that the forward process is a Markov chain, together with Bayes' rule, we can write

$$
\begin{equation}
\tag{6}

q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = q(\mathbf{x}_t \mid \mathbf{x}_{t-1}, \mathbf{x}_0) = q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) q(\mathbf{x}_t \mid \mathbf{x}_0) / q(\mathbf{x}_{t-1} \mid \mathbf{x}_0)

\end{equation}
$$

Plugging equation $$(6)$$ into $$(5)$$, we have:

$$
\begin{align}

L &= \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_{0})} \left[ \log \frac{q(\mathbf{x}_1 \mid \mathbf{x}_0)}{p(\mathbf{x}_{T}) p_{\theta}(\mathbf{x}_0 \mid \mathbf{x}_1)} \prod_{t=2}^{T} \frac{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) q(\mathbf{x}_t \mid \mathbf{x}_0)}{p_{\theta}(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}) q(\mathbf{x}_{t-1} \mid \mathbf{x}_0)} \right] \\
&= \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_{0})} \left[ \log \frac{q(\mathbf{x}_1 \mid \mathbf{x}_0)}{p(\mathbf{x}_{T}) p_{\theta}(\mathbf{x}_0 \mid \mathbf{x}_1)} \frac{q(\mathbf{x}_T \mid \mathbf{x}_0)}{q(\mathbf{x}_1 \mid \mathbf{x}_0)} \prod_{t=2}^{T} \frac{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)}{p_{\theta}(\mathbf{x}_{t-1} \mid \mathbf{x}_{t})} \right] \\
&= \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_{0})} \left[ \log \frac{q(\mathbf{x}_T \mid \mathbf{x}_0)}{p(\mathbf{x}_{T})} - \log p_{\theta}(\mathbf{x}_0 \mid \mathbf{x}_1) + \sum_{t=2}^{T} \log \frac{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)}{p_{\theta}(\mathbf{x}_{t-1} \mid \mathbf{x}_{t})} \right] \\
&= \underbrace{D_{\textrm{KL}}(q(\mathbf{x}_T \mid \mathbf{x}_0) \mid\mid p(\mathbf{x}_{T}))}_{L_T} \underbrace{- \mathbb{E}_{q(\mathbf{x}_1 \mid \mathbf{x}_{0})} \left[ \log p_{\theta}(\mathbf{x}_0 \mid \mathbf{x}_1) \right]}_{L_0} \\
&\quad + \sum_{t=2}^{T} \underbrace{\mathbb{E}_{q(\mathbf{x}_t \mid \mathbf{x}_{0})} \left[ D_{\textrm{KL}}(q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \mid\mid p_{\theta}(\mathbf{x}_{t-1} \mid \mathbf{x}_{t})) \right]}_{L_{t-1}} \tag{7}

\end{align}
$$

where $$D_{\textrm{KL}}$$ denotes the KL divergence. $$L_T$$ is a constant because $$q(\mathbf{x}_T \mid \mathbf{x}_0)$$ and $$p(\mathbf{x}_{T})$$ are fixed. $$L_0$$ and the $$L_{t-1}$$ terms penalize inaccurate denoising at different steps of the reverse process. Next, we'll derive a simple expression that allows us to minimize the $$L_{t-1}$$ terms. To do that, we'll first work out what $$q(\mathbf{x}_{t-1} \mid \mathbf{x}_0)$$ and $$q(\mathbf{x}_t \mid \mathbf{x}_0)$$ are, then use equation (6) to find $$q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)$$.

In the following, each of the $$\boldsymbol{\epsilon}$$ variables is distributed as $$\mathcal{N}(\mathbf{0}, \mathbf{I})$$. From equation $$(1)$$, we can write $$\mathbf{x}_1$$ as

$$
\begin{equation}
\tag{8}

\mathbf{x}_1 = \sqrt{\alpha_1} \mathbf{x}_0 + \sqrt{1 - \alpha_1} \boldsymbol{\epsilon}_1

\end{equation}
$$

Then, we can write $$\mathbf{x}_2$$ as

$$
\begin{align}

\mathbf{x}_2 &= \sqrt{\alpha_2} \mathbf{x}_1 + \sqrt{1 - \alpha_2} \boldsymbol{\epsilon}_2' \\
&= \sqrt{\alpha_2} (\sqrt{\alpha_1} \mathbf{x}_0 + \sqrt{1 - \alpha_1} \boldsymbol{\epsilon}_1) + \sqrt{1 - \alpha_2} \boldsymbol{\epsilon}_2' \\
&= \sqrt{\alpha_1 \alpha_2} \mathbf{x}_0 + \sqrt{1 - \alpha_1 \alpha_2} \boldsymbol{\epsilon}_2 \tag{9}

\end{align}
$$

In the last line, $$\sqrt{\alpha_2 (1 - \alpha_1)} \boldsymbol{\epsilon}_1$$ and $$\sqrt{1 - \alpha_2} \boldsymbol{\epsilon}_2'$$ have been replaced by $$\sqrt{1 - \alpha_1 \alpha_2} \boldsymbol{\epsilon}_2$$ because the sum of two normal random variables is another normal random variable, whose variance is the sum of the variances of the two original variables.

From equations $$(8)$$-$$(9)$$, we can see that $$\mathbf{x}_t$$ for any $$t$$ can be written as a weighted sum of $$\mathbf{x}_0$$ and a standard multivariate normal random variable $$\boldsymbol{\epsilon}$$:

$$
\begin{equation}
\tag{10}

\mathbf{x}_t (\mathbf{x}_0, \boldsymbol{\epsilon}) = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}

\end{equation}
$$

where $$\bar{\alpha}_t = \prod_{i=1}^t \alpha_i $$. Thus

$$
\begin{equation}
\tag{11}

q(\mathbf{x}_t \mid \mathbf{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t) \mathbf{I})

\end{equation}
$$

Note that if $$\bar{\alpha}_T$$ is close to $$0$$, then $$q(\mathbf{x}_T \mid \mathbf{x}_0)$$ is close to $$\mathcal{N}(\mathbf{0}, \mathbf{I})$$, justifying the choice of $$p(\mathbf{x}_{T}) = \mathcal{N}(\mathbf{0}, \mathbf{I})$$.

Plugging equations $$(1)$$ and $$(11)$$ into $$(6)$$, after some algebra and completing the square (see for example chapter 2.3 of Bishop (2016) for details), we find that

$$
\begin{align}

& q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) = \\
&\quad \mathcal{N}(\frac{\sqrt{\bar{\alpha}_{t-1}} (1 - \alpha_t)}{1 - \bar{\alpha}_t} \mathbf{x}_0 + \frac{\sqrt{\alpha_t} (1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t, \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} (1 - \alpha_t) \mathbf{I}) \tag{12}

\end{align}
$$

Thus, both $$q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)$$ and $$p_{\theta}(\mathbf{x}_{t-1} \mid \mathbf{x}_{t})$$ are Gaussians. The KL divergence between two $$n$$-dimensional Gaussians has the following closed form:

$$
\begin{align}

D_{\textrm{KL}}(\mathcal{N}(\boldsymbol{\mu}_q, \boldsymbol{\Sigma}_q) \mid\mid \mathcal{N}(\boldsymbol{\mu}_p, \boldsymbol{\Sigma}_p)) &= \frac{1}{2} [\textrm{tr}(\boldsymbol{\Sigma}_p^{-1} \boldsymbol{\Sigma}_q) - n + (\boldsymbol{\mu}_q - \boldsymbol{\mu}_p)^T \boldsymbol{\Sigma}_p^{-1} (\boldsymbol{\mu}_q - \boldsymbol{\mu}_p) \\
&\quad + \log \frac{\det \boldsymbol{\Sigma}_p}{\det \boldsymbol{\Sigma}_q}  ]

\end{align}
$$

Because $$q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)$$ and $$p_{\theta}(\mathbf{x}_{t-1} \mid \mathbf{x}_{t})$$ have fixed covariance matrices, minimizing $$D_{\textrm{KL}}(q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \mid\mid p_{\theta}(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}))$$ is equivalent to minimizing the following function, which penalizes the difference between the means of $$q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)$$ and $$p_{\theta}(\mathbf{x}_{t-1} \mid \mathbf{x}_{t})$$:

$$
\begin{equation}
\tag{13}

L_m = \frac{1}{2 \sigma^{2}_t} \left\| \frac{\sqrt{\bar{\alpha}_{t-1}} (1 - \alpha_t)}{1 - \bar{\alpha}_t} \mathbf{x}_0 + \frac{\sqrt{\alpha_t} (1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t - \boldsymbol{\mu}_{\theta}(\mathbf{x}_{t}, t) \right\|^2

\end{equation}
$$

Using equation $$(10)$$, we can rewrite this in terms of $$\boldsymbol{\epsilon}$$:

$$
\begin{equation}
\tag{14}

L_m = \frac{1}{2 \sigma^{2}_t} \left\| \frac{1}{\sqrt{\alpha_t}} \mathbf{x}_t (\mathbf{x}_0, \boldsymbol{\epsilon}) - \frac{1 - \alpha_t}{\sqrt{\alpha_t (1 - \bar{\alpha}_t)}} \boldsymbol{\epsilon} - \boldsymbol{\mu}_{\theta}(\mathbf{x}_{t} (\mathbf{x}_0, \boldsymbol{\epsilon}), t) \right\|^2

\end{equation}
$$

This motivates the following parameterization of $$\boldsymbol{\mu}_{\theta}(\mathbf{x}_{t}, t)$$:

$$
\begin{equation}
\tag{15}

\boldsymbol{\mu}_{\theta}(\mathbf{x}_{t}, t) = \frac{1}{\sqrt{\alpha_t}} \mathbf{x}_t (\mathbf{x}_0, \boldsymbol{\epsilon}) - \frac{1 - \alpha_t}{\sqrt{\alpha_t (1 - \bar{\alpha}_t)}} \boldsymbol{\epsilon}_{\theta} (\mathbf{x}_{t} (\mathbf{x}_0, \boldsymbol{\epsilon}), t)

\end{equation}
$$

where $$\boldsymbol{\epsilon}_{\theta} (\mathbf{x}_{t}, t)$$ is a neural network that predicts the noise $$\boldsymbol{\epsilon}$$ that needs to be removed from $$\mathbf{x}_{t}$$ to reconstruct $$\mathbf{x}_0$$. With this parameterization, $$L_m$$ becomes:

$$
\begin{equation}
\tag{16}

L_m = \gamma_t \left\| \boldsymbol{\epsilon}_{\theta} (\mathbf{x}_{t} (\mathbf{x}_0, \boldsymbol{\epsilon}), t) - \boldsymbol{\epsilon} \right\|^2, \quad \gamma_t = \frac{(1 - \alpha_t)^2}{2 \sigma^{2}_t \alpha_t (1 - \bar{\alpha}_t)}

\end{equation}
$$

With this, we see that we can minimize the $$L_{t-1}$$ terms of the negative ELBO (equation $$(7)$$) by minimizing the following loss

$$
\begin{equation}
\tag{17}

\mathbb{E}_{t \sim \textrm{uniform}(\{2,..., T \}), \: \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})} \left[ \gamma_t \left\| \boldsymbol{\epsilon}_{\theta} (\mathbf{x}_{t} (\mathbf{x}_0, \boldsymbol{\epsilon}), t) - \boldsymbol{\epsilon} \right\|^2 \right]

\end{equation}
$$

Based on this, Ho et al. (2020) introduced a simplified loss, where the terms for different $$t$$'s are weighted the same, and the $$t=1$$ step is included as well:

$$
\begin{equation}
\tag{18}

L_{\textrm{simple}} = \mathbb{E}_{t \sim \textrm{uniform}(\{1,..., T \}), \: \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})} \left[ \left\| \boldsymbol{\epsilon}_{\theta} (\mathbf{x}_{t} (\mathbf{x}_0, \boldsymbol{\epsilon}), t) - \boldsymbol{\epsilon} \right\|^2 \right]

\end{equation}
$$

While minimizing this loss is not equivalent to minimizing the negative ELBO, Ho et al. (2020) found that this works better empirically. To summarize, training a DDPM boils down to a very simple procedure: given data corrupted by noise, train a network to take a small denoising step by predicting the noise.

## Denoising diffusion implicit models

As we saw above, for a given DDPM, the steps of the reverse process are tied to the steps of the forward process. A large number of steps $$T$$ is usually needed to train a good model. For example, Ho et al. (2020) used $$T = 1000$$. This means 1000 sequential forward passes of the network are needed to generate a single sample, as opposed to one forward pass to generate one sample for other generative models like generative adversarial networks.

However, note that the DDPM loss $$L_{\textrm{simple}}$$ is only training a network to predict the noise in noise-corrupted data (or equivalently, to predict the uncorrupted data); it does not dictate that the noise-corrupted data has to be generated from a Markovian forward process, or that the network has to be used to take small denoising steps -- these are dictated by the specific latent variable model used in DDPM. To speed up sampling, Song et al. (2021) proposed DDIM, a diffusion model whose forward process is non-Markovian and for which sampling can be done with fewer steps. Crucially, they showed that $$L_{\textrm{simple}}$$ can also be used as the loss for DDIM -- this means that a DDPM network trained with a large $$T$$ can be directly used for fast sampling as a part of a DDIM without additional training.

Next, I'll first define the forward and reverse processes of DDIM, then derive its loss function. As before, $$\mathbf{x}_0$$ is our observed variable and $$\mathbf{x}_1$$, ..., $$\mathbf{x}_T$$ are the latent variables. Let $$\tau$$ be an increasing sub-sequence of $$[1,...,T]$$ of length $$S$$ with $$\tau_S = T$$, and $$\bar{\tau}$$ be the set of numbers in $$\{1,...,T\}$$ that are not in $$\tau$$. The non-Markovian forward process is

$$
\begin{equation}
\tag{19}

q'(\mathbf{x}_{1:T} \mid \mathbf{x}_0) = q'(\mathbf{x}_{\tau_S} \mid \mathbf{x}_0) \prod_{i=2}^{S} q'(\mathbf{x}_{\tau_{i-1}} \mid \mathbf{x}_{\tau_i}, \mathbf{x}_0) \prod_{t \in \bar{\tau}} q'(\mathbf{x}_t \mid \mathbf{x}_0)

\end{equation}
$$

where for $$t = T$$ and $$t \in \bar{\tau}$$,

$$
\begin{equation}
\tag{20}

q'(\mathbf{x}_t \mid \mathbf{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t) \mathbf{I})

\end{equation}
$$

and

$$
\begin{align}

& q'(\mathbf{x}_{\tau_{i-1}} \mid \mathbf{x}_{\tau_i}, \mathbf{x}_0) = \\
&\quad \mathcal{N}(\sqrt{\bar{\alpha}_{\tau_{i-1}}} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_{\tau_{i-1}} - k^2_{\tau_i}} \frac{\mathbf{x}_{\tau_i} - \sqrt{\bar{\alpha}_{\tau_i}} \mathbf{x}_0}{\sqrt{1 - \bar{\alpha}_{\tau_i}}}, k^2_{\tau_i} \mathbf{I}) \tag{21}

\end{align}
$$

The $$k_{\tau_i}$$'s are hyperparameters. These choices of $$q'(\mathbf{x}_t \mid \mathbf{x}_0)$$ and $$q'(\mathbf{x}_{\tau_{i-1}} \mid \mathbf{x}_{\tau_i}, \mathbf{x}_0)$$ ensure that, for all $$t$$,

$$
\begin{equation}
\tag{22}

q'(\mathbf{x}_t \mid \mathbf{x}_0) = q(\mathbf{x}_t \mid \mathbf{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t) \mathbf{I})

\end{equation}
$$

This is true for $$t \in \bar{\tau}$$ by definition. For $$t$$'s that are part of the $$\tau$$ sub-sequence, we can use induction to show that it's also true. Suppose $$q'(\mathbf{x}_{\tau_i} \mid \mathbf{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha}_{\tau_i}} \mathbf{x}_0, (1 - \bar{\alpha}_{\tau_i}) \mathbf{I})$$ for some $$\tau_i$$, then

$$
\begin{align}

q'(\mathbf{x}_{\tau_{i-1}} \mid \mathbf{x}_0) &= \int q'(\mathbf{x}_{\tau_i} \mid \mathbf{x}_0) q'(\mathbf{x}_{\tau_{i-1}} \mid \mathbf{x}_{\tau_i}, \mathbf{x}_0) d\mathbf{x}_{\tau_i} \\
&= \mathcal{N}(\sqrt{\bar{\alpha}_{\tau_{i-1}}} \mathbf{x}_0, (1 - \bar{\alpha}_{\tau_{i-1}}) \mathbf{I})

\end{align}
$$

where the last line involves some manipulations of Gaussians (see for example chapter 2.3 of Bishop (2016) for details). Now, starting with the base case $$t = \tau_S = T$$, for which equation $$(22)$$ is true by definition, the above induction step shows that equation $$(22)$$ is true for all $$t$$'s that are part of the $$\tau$$ sub-sequence.

The reverse process starts at $$p'(\mathbf{x}_{T}) = \mathcal{N}(\mathbf{0}, \mathbf{I})$$, and the joint distribution is

$$
\begin{equation}
\tag{23}

p'_{\theta}(\mathbf{x}_{0:T}) = p'(\mathbf{x}_{T}) \prod_{i=2}^{S} p'_{\theta}(\mathbf{x}_{\tau_{i-1}} \mid \mathbf{x}_{\tau_i}) p'_{\theta}(\mathbf{x}_0 \mid \mathbf{x}_{\tau_1}) \prod_{t \in \bar{\tau}} p'(\mathbf{x}_t \mid \mathbf{x}_0)

\end{equation}
$$

where

$$
\begin{align}

& p'_{\theta}(\mathbf{x}_{\tau_{i-1}} \mid \mathbf{x}_{\tau_i}) = q'(\mathbf{x}_{\tau_{i-1}} \mid \mathbf{x}_{\tau_i}, \boldsymbol{f}_{\theta}(\mathbf{x}_{\tau_i}, {\tau_i})) \tag{24} \\

& p'_{\theta}(\mathbf{x}_0 \mid \mathbf{x}_{\tau_1}) = \mathcal{N}(\boldsymbol{f}_{\theta}(\mathbf{x}_{\tau_1}, {\tau_1}), k^2_{\tau_1} \mathbf{I}) \tag{25} \\

& p'(\mathbf{x}_t \mid \mathbf{x}_0) = q'(\mathbf{x}_t \mid \mathbf{x}_0) \ \textrm{for} \ t \in \bar{\tau} \tag{26}

\end{align}
$$

Here, $$\boldsymbol{f}_{\theta}(\mathbf{x}_t, t)$$ is the model's prediction of $$\mathbf{x}_0$$ given $$\mathbf{x}_t$$. Based on equation $$(10)$$, we parameterize $$\boldsymbol{f}_{\theta}(\mathbf{x}_t, t)$$ as

$$
\begin{equation}
\tag{27}

\boldsymbol{f}_{\theta}(\mathbf{x}_t, t) = \frac{\mathbf{x}_t (\mathbf{x}_0, \boldsymbol{\epsilon}) - \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}_{\theta} (\mathbf{x}_{t} (\mathbf{x}_0, \boldsymbol{\epsilon}), t)}{\sqrt{\bar{\alpha}_t}}

\end{equation}
$$

where, as in DDPM, $$\boldsymbol{\epsilon}_{\theta} (\mathbf{x}_{t}, t)$$ is a neural network that predicts the noise $$\boldsymbol{\epsilon}$$.

Note that with the reverse process of equation $$(23)$$, we don't need to sample all the latent variables when generating samples $$\mathbf{x}_0$$; instead, we only need to use the Markov chain defined by $$p'(\mathbf{x}_{T}) \prod_{i=2}^{S} p'_{\theta}(\mathbf{x}_{\tau_{i-1}} \mid \mathbf{x}_{\tau_i}) p'_{\theta}(\mathbf{x}_0 \mid \mathbf{x}_{\tau_1})$$ to sample the latent variables indexed by $$\tau$$. Thus, to generate one sample, we only need to run $$S$$ rather than $$T$$ sequential forward passes of the $$\boldsymbol{\epsilon}_{\theta}$$ network. Furthermore, we can set the hyperparameters $$k_{\tau_i}$$'s to $$0$$, so that sampling is deterministic given $$\mathbf{x}_{T}$$.

Now, let's derive the loss function for this model. In the following, I'll use various $$C$$ variables to absorb terms that do not depend on the parameters $$\theta$$. As before, we start with the negative ELBO:

$$
\begin{align}

L' &= \mathbb{E}_{q'(\mathbf{x}_{1:T} \mid \mathbf{x}_{0})} \left[ \log \frac{q'(\mathbf{x}_{1:T} \mid \mathbf{x}_{0})}{p'_{\theta}(\mathbf{x}_{0:T})} \right] \\

&= \mathbb{E}_{q'(\mathbf{x}_{1:T} \mid \mathbf{x}_{0})} \left[ \log \frac{q'(\mathbf{x}_{\tau_S} \mid \mathbf{x}_0) \prod_{i=2}^{S} q'(\mathbf{x}_{\tau_{i-1}} \mid \mathbf{x}_{\tau_i}, \mathbf{x}_0) \prod_{t \in \bar{\tau}} q'(\mathbf{x}_t \mid \mathbf{x}_0)}{p'(\mathbf{x}_{T}) \prod_{i=2}^{S} p'_{\theta}(\mathbf{x}_{\tau_{i-1}} \mid \mathbf{x}_{\tau_i}) p'_{\theta}(\mathbf{x}_0 \mid \mathbf{x}_{\tau_1}) \prod_{t \in \bar{\tau}} p'(\mathbf{x}_t \mid \mathbf{x}_0)} \right] \\

&= \mathbb{E}_{q'(\mathbf{x}_{1:T} \mid \mathbf{x}_{0})} \left[ \sum_{i=2}^{S} \log \frac{q'(\mathbf{x}_{\tau_{i-1}} \mid \mathbf{x}_{\tau_i}, \mathbf{x}_0)}{p'_{\theta}(\mathbf{x}_{\tau_{i-1}} \mid \mathbf{x}_{\tau_i})} - \log p'_{\theta}(\mathbf{x}_0 \mid \mathbf{x}_{\tau_1}) \right] + C_1 \\

&= \sum_{i=2}^{S} \underbrace{\mathbb{E}_{q'(\mathbf{x}_{\tau_i} \mid \mathbf{x}_{0})} \left[ D_{\textrm{KL}} ( q'(\mathbf{x}_{\tau_{i-1}} \mid \mathbf{x}_{\tau_i}, \mathbf{x}_0) \mid\mid p'_{\theta}(\mathbf{x}_{\tau_{i-1}} \mid \mathbf{x}_{\tau_i}) ) \right]}_{L'_{i-1}} \\
&\qquad \underbrace{- \mathbb{E}_{q'(\mathbf{x}_{\tau_1} \mid \mathbf{x}_{0})} \left[ \log p'_{\theta}(\mathbf{x}_0 \mid \mathbf{x}_{\tau_1}) \right]}_{L'_0} + C_1 \tag{28}

\end{align}
$$

Similar to the $$L_{t-1}$$ terms of DDPM, $$L'_{i-1}$$ compares two Gaussians (equations $$(21)$$ and $$(24)$$) with fixed covariance matrices. Thus, similar to equation $$(13)$$, $$L'_{i-1}$$ reduces to

$$
\begin{align}

L'_{i-1} &= \mathbb{E}_{q'(\mathbf{x}_{\tau_i} \mid \mathbf{x}_{0})} \left[ \frac{1}{2 k^2_{\tau_i}} \left\| (\sqrt{\bar{\alpha}_{\tau_{i-1}}} - \frac{\sqrt{1 - \bar{\alpha}_{\tau_{i-1}} - k^2_{\tau_i}} \sqrt{\bar{\alpha}_{\tau_i}}}{\sqrt{1 - \bar{\alpha}_{\tau_i}}} )(\mathbf{x}_{0} - \boldsymbol{f}_{\theta}(\mathbf{x}_{\tau_i}, {\tau_i})) \right\|^2 \right] \\
&\qquad + C_2 \\

&= \mathbb{E}_{\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})} \left[ \gamma_{\tau_i} \left\| \boldsymbol{\epsilon}_{\theta} (\mathbf{x}_{\tau_i} (\mathbf{x}_0, \boldsymbol{\epsilon}), \tau_i) - \boldsymbol{\epsilon} \right\|^2 \right] + C_2 \tag{29}

\end{align}
$$

with

$$
\begin{equation}
\tag{30}

\gamma_{\tau_i} = \frac{1}{2 k^2_{\tau_i}} (\frac{\sqrt{\bar{\alpha}_{\tau_{i-1}}(1 - \bar{\alpha}_{\tau_i})}}{\sqrt{\bar{\alpha}_{\tau_i}}} - \sqrt{1 - \bar{\alpha}_{\tau_{i-1}} - k^2_{\tau_i}})^2

\end{equation}
$$

On the other hand, plugging in equation $$(25)$$, $$L'_0$$ becomes

$$
\begin{align}

L'_0 &= -\mathbb{E}_{q'(\mathbf{x}_{\tau_1} \mid \mathbf{x}_{0})} \left[ - \frac{1}{2k^2_{\tau_1}} \left\| \mathbf{x}_0 - \boldsymbol{f}_{\theta}(\mathbf{x}_{\tau_1}, {\tau_1}) \right\|^2 \right] + C_3 \\

&= \mathbb{E}_{\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})} \left[ \gamma_{\tau_1} \left\| \boldsymbol{\epsilon}_{\theta} (\mathbf{x}_{\tau_1} (\mathbf{x}_0, \boldsymbol{\epsilon}), \tau_1) - \boldsymbol{\epsilon} \right\|^2 \right] + C_3 \tag{31}

\end{align}
$$

with

$$
\begin{equation}
\tag{32}

\gamma_{\tau_1} = \frac{1}{2 k^2_{\tau_1}} \frac{1 - \bar{\alpha}_{\tau_1}}{\bar{\alpha}_{\tau_1}}

\end{equation}
$$

As we can see, the terms of $$L'$$ have the form of equation $$(16)$$, with different values for the $$\gamma_t$$'s: equations $$(30)$$ and $$(32)$$ gives $$\gamma_t$$ for $$t$$'s that are part of $$\tau$$, and $$\gamma_t = 0$$ for $$t \in \bar{\tau}$$. Thus, $$L_{\textrm{simple}}$$ (equation $$(18)$$), used for training DDPM, is also a reasonable surrogate loss for DDIM. In other words, a denoising network $$\boldsymbol{\epsilon}_{\theta} (\mathbf{x}_{t}, t)$$ trained with $$L_{\textrm{simple}}$$ as part of a DDPM can be plugged straight into a DDIM for fast sampling.


## References

Bishop, C. (2006). *Pattern recognition and machine learning*.

Ho, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. *NeurIPS*.

Song, J., Meng, C., & Ermon, S. (2021). Denoising diffusion implicit models. *ICLR*.
